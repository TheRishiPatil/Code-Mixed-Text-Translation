{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":763,"status":"ok","timestamp":1723611434486,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"YxlAbIVpnxNQ"},"outputs":[],"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import re\n","import random\n","import pickle\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Loading data files"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1723611435396,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"jIbva_QFn2RP"},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1723611435397,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"p8OMI1Yvn8Od"},"outputs":[],"source":["# Turn a Unicode string to plain ASCII\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n","    return s.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1723611435397,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"tyv36ldRn-zt"},"outputs":[],"source":["def readLangs(lang1, lang2, reverse=False):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines = open('whole_data.txt', encoding='utf-8').\\\n","        read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize, handling potential missing delimiters\n","    pairs = []\n","    for l in lines:\n","        parts = l.split('\\t')\n","        if len(parts) == 2:  # Check if the line was split into two parts\n","            pairs.append([normalizeString(s) for s in parts])\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang(lang2)\n","        output_lang = Lang(lang1)\n","    else:\n","        input_lang = Lang(lang1)\n","        output_lang = Lang(lang2)\n","\n","    return input_lang, output_lang, pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1723611435397,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"0__feOC2hX6O"},"outputs":[],"source":["def prepareData(lang1, lang2, reverse=False): # Define the missing function\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    return input_lang, output_lang, pairs # Add return statement to return the variables"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":930,"status":"ok","timestamp":1723611436317,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"FUYXQ7oqoAyN","outputId":"e125db80-a053-4c04-e6ea-fd8cfc600d76"},"outputs":[],"source":["MAX_LENGTH = 10\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH\n","        # Removed the prefix requirement to include more pairs\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","# ... rest of the code ...\n","\n","input_lang, output_lang, pairs = prepareData('eng', 'mar', True)\n","if pairs: # Check if pairs is not empty before trying to select a random element\n","    print(random.choice(pairs))\n","else:\n","    print(\"No pairs found after filtering.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2501,"status":"ok","timestamp":1723611442981,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"fHeGOZpdoCd9","outputId":"a3d541f6-0402-4ce0-ba03-96d1b1cb4765"},"outputs":[],"source":["def prepareData(lang1, lang2, reverse=False):\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('eng', 'mar', True)\n","print(random.choice(pairs))"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":75,"status":"ok","timestamp":1723611442981,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"leLOqcIW1G35"},"source":["## The Seq2Seq Model"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":73,"status":"ok","timestamp":1723611442982,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"QhzK0R_e1Ljp"},"source":["## The Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":71,"status":"ok","timestamp":1723611442982,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"m5vTSKqzoEL9"},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input):\n","        embedded = self.dropout(self.embedding(input))\n","        output, hidden = self.gru(embedded)\n","        return output, hidden"]},{"cell_type":"markdown","metadata":{},"source":["## The Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":68,"status":"ok","timestamp":1723611442982,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"-4f3GZljoxDU"},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n","            decoder_outputs.append(decoder_output)\n","\n","            if target_tensor is not None:\n","                # Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1) # forcing\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n","\n","    def forward_step(self, input, hidden):\n","        output = self.embedding(input)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.out(output)\n","        return output, hidden"]},{"cell_type":"markdown","metadata":{},"source":["## Attention Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":67,"status":"ok","timestamp":1723611442983,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"LBjqaWhtoxA0"},"outputs":[],"source":["class BahdanauAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(BahdanauAttention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze(2).unsqueeze(1)\n","\n","        weights = F.softmax(scores, dim=-1)\n","        context = torch.bmm(weights, keys)\n","\n","        return context, weights\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.attention = BahdanauAttention(hidden_size)\n","        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n","        batch_size = encoder_outputs.size(0)\n","        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n","        decoder_hidden = encoder_hidden\n","        decoder_outputs = []\n","        attentions = []\n","\n","        for i in range(MAX_LENGTH):\n","            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            decoder_outputs.append(decoder_output)\n","            attentions.append(attn_weights)\n","\n","            if target_tensor is not None:\n","                # Teacher forcing: Feed the target as the next input\n","                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n","            else:\n","                # Without teacher forcing: use its own predictions as the next input\n","                _, topi = decoder_output.topk(1)\n","                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n","\n","        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n","        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n","        attentions = torch.cat(attentions, dim=1)\n","\n","        return decoder_outputs, decoder_hidden, attentions\n","\n","\n","    def forward_step(self, input, hidden, encoder_outputs):\n","        embedded =  self.dropout(self.embedding(input))\n","\n","        query = hidden.permute(1, 0, 2)\n","        context, attn_weights = self.attention(query, encoder_outputs)\n","        input_gru = torch.cat((embedded, context), dim=2)\n","\n","        output, hidden = self.gru(input_gru, hidden)\n","        output = self.out(output)\n","\n","        return output, hidden, attn_weights"]},{"cell_type":"markdown","metadata":{},"source":["## Preparing Training Data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":65,"status":"ok","timestamp":1723611442983,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"IQakWolBow-E"},"outputs":[],"source":["def indexesFromSentence(lang, sentence):\n","    return [lang.word2index.get(word.lower(), 0) for word in sentence.split(' ')]\n","    # Use .get() with a default value (UNK_token) to handle missing words\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","def get_dataloader(batch_size):\n","    input_lang, output_lang, pairs = prepareData('eng', 'mar', True)\n","\n","    n = len(pairs)\n","    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n","\n","    for idx, (inp, tgt) in enumerate(pairs):\n","        inp_ids = indexesFromSentence(input_lang, inp)\n","        tgt_ids = indexesFromSentence(output_lang, tgt)\n","        inp_ids.append(EOS_token)\n","        tgt_ids.append(EOS_token)\n","        input_ids[idx, :len(inp_ids)] = inp_ids\n","        target_ids[idx, :len(tgt_ids)] = tgt_ids\n","\n","    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n","                               torch.LongTensor(target_ids).to(device))\n","\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","    return input_lang, output_lang, train_dataloader"]},{"cell_type":"markdown","metadata":{},"source":["## Training the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":64,"status":"ok","timestamp":1723611442984,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"16af7qavow7c"},"outputs":[],"source":["def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n","          decoder_optimizer, criterion):\n","\n","    total_loss = 0\n","    for data in dataloader:\n","        input_tensor, target_tensor = data\n","\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n","\n","        loss = criterion(\n","            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n","            target_tensor.view(-1)\n","        )\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":62,"status":"ok","timestamp":1723611442984,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"wNs7miS1ow30"},"outputs":[],"source":["import time\n","import math\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":61,"status":"ok","timestamp":1723611442984,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"pItA1d2Mo6f7"},"outputs":[],"source":["def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n","               print_every=100, plot_every=100):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(1, n_epochs + 1):\n","        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if epoch % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n","                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n","\n","        if epoch % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)"]},{"cell_type":"markdown","metadata":{},"source":["## Plotting results"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":60,"status":"ok","timestamp":1723611442985,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"g7dydvE3o6dM"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":58,"status":"ok","timestamp":1723611442985,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"7GX7yQQZo6aU"},"outputs":[],"source":["def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentence)\n","\n","        encoder_outputs, encoder_hidden = encoder(input_tensor)\n","        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n","\n","        _, topi = decoder_outputs.topk(1)\n","        decoded_ids = topi.squeeze()\n","\n","        decoded_words = []\n","        for idx in decoded_ids:\n","            if idx.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            decoded_words.append(output_lang.index2word[idx.item()])\n","    return decoded_words, decoder_attn"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1723611442985,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"WQGAK-Bgo6XM"},"outputs":[],"source":["def evaluateRandomly(encoder, decoder, n=10):\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')"]},{"cell_type":"markdown","metadata":{},"source":["## Training and Evaluating"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1582589,"status":"ok","timestamp":1723613025519,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"EJjz-Bz7o_c0","outputId":"40c3914c-9c93-4a40-b82e-a5346a207e79"},"outputs":[],"source":["hidden_size = 256\n","batch_size = 64\n","\n","input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n","\n","encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n","\n","train(train_dataloader, encoder, decoder, 150, print_every=5, plot_every=5) # Increased epochs to 150"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88,"status":"ok","timestamp":1723613025519,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"RUCtbI2No_Z9","outputId":"05b07acd-b580-40b5-b1d7-180c14f0491d"},"outputs":[],"source":["encoder.eval()\n","decoder.eval()\n","evaluateRandomly(encoder, decoder)"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":929,"status":"ok","timestamp":1723613158128,"user":{"displayName":"YoKuRRR","userId":"15180566003688590481"},"user_tz":-330},"id":"OQmsLLzuo_XL","outputId":"aa2bfe99-189e-469b-992d-5f7b72875f61"},"outputs":[],"source":["def evaluateAndShowAttention(input_sentence):\n","    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n","    print('input =', input_sentence)\n","    print('output =', ' '.join(output_words))\n","\n","\n","\n","evaluateAndShowAttention('ti la turn karayla koi nahi hota')\n","evaluateAndShowAttention('tu tire cha pressure check karu shakto ka ?')\n","evaluateAndShowAttention('to radio aikat basla')\n","evaluateAndShowAttention('kona hi mala aikto ka ?')\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
